{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob('*.txt')\n",
    "\n",
    "words = []\n",
    "for f in files:\n",
    "    file = open(f)\n",
    "    words.append(file.read())\n",
    "    file.close()\n",
    "\n",
    "words = list(chain.from_iterable(words))\n",
    "words = ''.join(words)[:-1]\n",
    "sentences = words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 97415], ('the', 667723), ('and', 324431), ('a', 322940), ('of', 289409)]\n",
      "Sample data [[279, 174, 436, 7048, 46, 3381, 43, 3, 15, 16, 223, 1129, 72, 1707, 1166, 37, 1, 1308, 1603, 2144, 836, 4, 61, 39, 25, 53, 172, 9, 41, 117, 23, 453, 44, 99, 4, 1, 101, 89, 4, 178, 27, 2991, 8, 1, 4180, 2, 7048, 12, 104, 25, 1654, 399, 22, 2, 92, 1501, 364, 73, 304, 33, 61, 57, 9, 117, 23, 453, 1, 104, 70, 142, 64, 453, 44, 6, 3, 53, 9629, 34013, 9799, 9168, 1, 444, 6, 24, 269, 122, 14, 516, 35, 1237, 24, 12, 127, 72, 244, 322, 184, 86, 2, 273, 56, 3794, 4, 3, 17093, 4180, 26, 62, 13182, 721, 5, 29, 1705, 121, 7048, 414, 51, 70, 25, 69, 501, 1, 302, 95, 218, 4, 10, 3890, 7048, 704, 178, 30, 44, 9168, 12, 2991, 70, 25, 556, 134, 9168, 6, 2098, 5, 29, 1, 118, 17, 56, 2521, 15410, 40, 11272, 56, 1314, 129, 7, 13, 30, 9, 96, 78, 5, 386, 37, 1595, 7, 122, 34, 519, 8], [10, 6, 34, 464, 4, 134, 1, 2219, 4, 205, 105, 25, 1, 168, 3854, 2, 347, 39, 12, 64, 163, 276, 149, 129, 3, 589, 447, 4, 1, 92, 1163, 14102, 1993, 4, 1774, 21, 2, 1774, 4935, 35, 137, 251, 12992, 107, 209, 121, 11, 32, 25, 2173, 4, 114, 2, 114, 69, 87, 21, 1246, 17, 10, 28, 141, 65, 169, 652, 518, 31314, 40, 103, 169, 835, 6074, 16, 1774, 21, 40, 18430, 1489, 1, 2802, 1980, 2826, 40, 2380, 16, 1774, 4935, 2, 65, 1, 145, 781, 1774, 21, 12, 2242, 7103, 408, 610, 164, 10, 19, 16851, 31, 1, 2650, 2, 9, 139, 131, 1528, 47, 1, 2133, 942, 5702, 13, 400, 8, 10, 19, 2, 134, 1, 2133, 125, 24, 212, 285, 1, 2528, 168, 104, 37, 2014, 18354, 171, 19, 9, 137, 108, 17, 942, 5702, 46, 86, 392, 1, 2528, 168, 2040, 104, 2, 31, 222, 8, 2014, 26, 104, 1061, 61, 90, 7, 661, 15980, 435, 10, 6, 334, 950, 205, 1140, 39, 25, 3546, 127, 105, 5, 65, 2, 45, 20, 64, 180, 5, 65, 10, 28, 103, 7836, 308, 61, 6, 2393, 3, 10674, 940, 18, 46, 127, 114, 2, 3, 127, 225, 1, 62, 148, 11, 90, 10, 31, 30, 276, 149, 13, 3, 530, 515, 22, 1, 349, 1, 619, 13, 216, 2509, 61, 269, 493, 5, 231, 55, 16, 1, 486, 19, 388, 18, 23, 181]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 40000\n",
    "\n",
    "def build_dataset(sentences):\n",
    "    words = ''.join(sentences).split()\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    unk_count = 0\n",
    "    sent_data = []\n",
    "    for sentence in sentences:\n",
    "        data = []\n",
    "        for word in sentence.split():\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "        sent_data.append(data)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return sent_data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(sentences)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:2])\n",
    "# del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11408957\n"
     ]
    }
   ],
   "source": [
    "skip_window = 3\n",
    "instances = 0\n",
    "for sentence  in data:\n",
    "    instances += len(sentence)-2*skip_window\n",
    "print(instances)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context = np.zeros((instances,skip_window*2),dtype=np.int32)\n",
    "labels = np.zeros((instances,1),dtype=np.int32)\n",
    "doc = np.zeros((instances,1),dtype=np.int32)\n",
    "\n",
    "k = 0\n",
    "for doc_id, sentence  in enumerate(data):\n",
    "    for i in range(skip_window, len(sentence)-skip_window):\n",
    "        buffer = sentence[i-skip_window:i+skip_window+1]\n",
    "        del buffer[skip_window]\n",
    "        labels[k] = sentence[i]\n",
    "        context[k] = buffer\n",
    "        doc[k] = doc_id\n",
    "        k += 1\n",
    "        \n",
    "shuffle_idx = np.random.permutation(k)\n",
    "labels = labels[shuffle_idx]\n",
    "doc = doc[shuffle_idx]\n",
    "context = context[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "context_window = 2*skip_window\n",
    "embedding_size = 50 # Dimension of the embedding vector.\n",
    "softmax_width = embedding_size # +embedding_size2+embedding_size3\n",
    "num_sampled = 100 # Number of negative examples to sample.\n",
    "sum_ids = np.repeat(np.arange(batch_size),context_window)\n",
    "\n",
    "len_docs = len(data)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(): # , tf.device('/cpu:0')\n",
    "    # Input data.\n",
    "    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size*context_window])\n",
    "    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    segment_ids = tf.constant(sum_ids, dtype=tf.int32)\n",
    "\n",
    "    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n",
    "\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, softmax_width],\n",
    "                             stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed_words = tf.segment_sum(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),segment_ids)\n",
    "    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n",
    "    embed = embed_words+embed_docs#+embed_hash+embed_users\n",
    "\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(softmax_weights, softmax_biases, embed,\n",
    "                                   train_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "        \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(doc_embeddings), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = doc_embeddings / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Chunk the data to be passed into the tensorflow Model\n",
    "###########################\n",
    "data_idx = 0\n",
    "def generate_batch(batch_size):\n",
    "    global data_idx\n",
    "\n",
    "    if data_idx+batch_size<len_docs:\n",
    "        batch_labels = labels[data_idx:data_idx+batch_size]\n",
    "        batch_doc_data = doc[data_idx:data_idx+batch_size]\n",
    "        batch_word_data = context[data_idx:data_idx+batch_size]\n",
    "        data_idx += batch_size\n",
    "    else:\n",
    "        overlay = batch_size - (len_docs-data_idx)\n",
    "        batch_labels = np.vstack([labels[data_idx:len_docs],labels[:overlay]])\n",
    "        batch_doc_data = np.vstack([doc[data_idx:len_docs],doc[:overlay]])\n",
    "        batch_word_data = np.vstack([context[data_idx:len_docs],context[:overlay]])\n",
    "        data_idx = overlay\n",
    "    batch_word_data = np.reshape(batch_word_data,(-1,1))\n",
    "\n",
    "    return batch_labels, batch_word_data, batch_doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 426.157104\n",
      "Average loss at step 50000: 7.854996\n",
      "Average loss at step 100000: 0.641150\n",
      "Average loss at step 150000: 0.603961\n",
      "Average loss at step 200000: 0.594481\n",
      "Average loss at step 250000: 0.589963\n",
      "Average loss at step 300000: 0.586765\n",
      "Average loss at step 350000: 0.584712\n",
      "Average loss at step 400000: 0.583086\n",
      "Average loss at step 450000: 0.582070\n",
      "Average loss at step 500000: 0.581260\n",
      "Average loss at step 550000: 0.580287\n",
      "Average loss at step 600000: 0.579365\n",
      "Average loss at step 650000: 0.579081\n",
      "Average loss at step 700000: 0.578365\n",
      "Average loss at step 750000: 0.578061\n",
      "Average loss at step 800000: 0.577644\n",
      "Average loss at step 850000: 0.577144\n",
      "Average loss at step 900000: 0.576824\n",
      "Average loss at step 950000: 0.576325\n",
      "Average loss at step 1000000: 0.576282\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1000001\n",
    "step_delta = int(num_steps/20)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_labels, batch_word_data, batch_doc_data\\\n",
    "        = generate_batch(batch_size)\n",
    "        feed_dict = {train_word_dataset : np.squeeze(batch_word_data),\n",
    "                     train_doc_dataset : np.squeeze(batch_doc_data),\n",
    "                     train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % step_delta == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / step_delta\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "    # Get the weights to save for later\n",
    "#     final_doc_embeddings = normalized_doc_embeddings.eval()\n",
    "    final_word_embeddings = word_embeddings.eval()\n",
    "    final_word_embeddings_out = softmax_weights.eval()\n",
    "    final_doc_embeddings = normalized_doc_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.539897\n",
      "0.535443\n",
      "0.523045\n"
     ]
    }
   ],
   "source": [
    "rand_doc = np.random.randint(len_docs)\n",
    "dist = final_doc_embeddings.dot(final_doc_embeddings[rand_doc][:,None])\n",
    "closest_doc = np.argsort(dist,axis=0)[-4:][::-1]\n",
    "\n",
    "for idx in closest_doc:\n",
    "    print(dist[idx][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love this movie beautifully funny and utterly believable characters each scene richer and more wonderful than the last every aspect of this movie is filled with wit and humour and love and depth a complex and engrossing story too this movie is filled with love humour and intelligence totally great '"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[rand_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for many the hit series was ten years of pitch black humour loaded with affectionate parodies of classic films and a hilarious assortment of over a hundred characters with instantly recognisable catchphrases few shows have survived transition from radio to tv to stage show to film but the league of gentlemen have achieved it with suitable aplomb the talented writer performers had initially envisioned a monty python style medieval adventure but as soon as writing began they soon realised that the characters they have lived with had become very real and deserved better with that the royston vasey folk realise their very existence is under threat as the writers decide to disregard the fictitious town and work on a th century romp instead with the exception of michael sheen playing much unseen league member jeremy dyson the league play pretty unlikeable caricatures of their real life personae as well as the familiar faces of tubbs i made a little brown fishy nightmare inducing sexual predator herr lipp butcher hilary briss and an unlikely hero irate businessman geoff tibbs new faces appear when the third reality appears it s here we are treated to charming and funny cameos from veteran actors and popular tv stars for many this will be a really enjoyable minutes apocalpse is not going to please everyone though working on this level of post modernism has been done a few times before now and may seem all too familiar to audiences raised on irony drenched teen successes kick started by the likes of wes craven having a new nightmare it also takes a lot of confidence in an audience to keep up with a high concept story so there are moments of exposition and dialogue that serve only to confirm what most in the audience already know comedy as a genre is formulaic but it s now unheard of for a british film not to fall back on the huge back catalogue of tv stars to fill short amounts of screen time it s also hard to believe the creators ever wanted their offspring killed off which is perhaps why some of the role reversal doesn t always quite hit the mark would hilary briss have wanted to try save royston vasey in the series however while the show s deliciously dark vein has almost all but disappeared but is arguably more accessible for it much will be said about the character development and efforts to humanise the likes of previously one joke incarnations like herr lipp it is here an impossible level of depth can be found along with a harsh streak of biting satire and throwaway put downs sentiment is there with a lump in the throat but not sugar coated thickly enough to intrude on the action the music is good performances exemplary and the animation is wonderfully seamless a nice throwback to terry gilliam and ray harryhausen s work in short there s a lot to like about the apocalypse like so many tv to film transfers it was never going to be easy finding the line between preaching to the converted and introducing the uninitiated to the league s slick and distinct voice but no matter what your preference is this last trip to the town which you ll never leave is oddly lined with hope and ultimately very very touching '"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc[1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my wife and i watched this marvelous movie this evening because we will watch russian dolls tomorrow and the first is important to see before the second we both loved the spaniah apartment and will enjoy following some of the characters through the early years of their lives now in st pertersburg russia we both identified slightly with the rough framework of the story because we were students florence italy for us so the script was not completely foreign to our early lives our living was considerably different but as with this movie anytime you throw together young people passing through the same life hoops as any developed world people they will experience much the same life situations the collection of people and the personal difficulties that they faced were universals and therein lies the beauty of this film and probably its sequence that i will see tomorrow as i wrote the characters as well as these situations are familiar to all of us and therefore we can enjoy living their lives for awhile this must be one of the film s great strengths allowing the viewer to vicariously experience the emotional upheavals of the people involved and yet remain aloof the viewer can through that distance chuckle to themselves thinking i wouldn t have done something that dumb or i would have avoided that trap maybe that s why we go to movies '"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc[2][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'despite myself i really kinda like this movie pauley shore is invariably laugh out loud funny and here is no exception he is just excellent at playing the weirdo with a heart of gold his performance in this although nothing out of the ordinary for him is so good it seems to lift other cast members performances perhaps this is because he s the kind of guy it s easy to bounce off of the clich s about country life in this movie are hilarious and the way shore s city boy crawl is so at odds with the way of life is funny too but it s not only he who s a fish out of water comedy also comes from the fact that to any ordinary person or people crawl is a freakish nightmare of a person that s why this movie works in such a great way we love crawl he s a breath of fresh air but we can also sympathise with the warners he is one hell of a culture shock although this movie is classic pauley shore so there s no great brain power needed to enjoy the movie enjoy it you do and there s even a never judge a book by it s cover type moral here somewhere not bad not bad at all '"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc[3][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAFkCAYAAABIPLOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQZIkZTM4SJKkbAYHSZKUzeAgSZKy/X/kUz+ZR1DU0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x130f70fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(dist,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "cpus = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    for i,sentence in enumerate(words.split('\\n')):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(sentence), [i])\n",
    "\n",
    "train_corpus = list(read_corpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(dm=1, dm_concat=0, size=embedding_size, window=skip_window, \n",
    "                negative=num_sampled,hs=0, min_count=5, workers=cpus, iter=10)\n",
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.train(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

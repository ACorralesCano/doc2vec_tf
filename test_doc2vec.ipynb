{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import glob\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob('*.txt')\n",
    "\n",
    "words = []\n",
    "for f in files:\n",
    "    file = open(f)\n",
    "    words.append(file.read())\n",
    "    file.close()\n",
    "\n",
    "words = list(chain.from_iterable(words))\n",
    "words = ''.join(words)[:-1]\n",
    "sentences = words.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNK', -1],\n",
       " ('the', 667723),\n",
       " ('and', 324431),\n",
       " ('a', 322940),\n",
       " ('of', 289409),\n",
       " ('to', 268111),\n",
       " ('is', 211076),\n",
       " ('it', 190624),\n",
       " ('in', 186745),\n",
       " ('i', 175021),\n",
       " ('this', 150651)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = [['UNK', -1]]\n",
    "count.extend(collections.Counter(''.join(sentences).split()).most_common(10))\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 63666], ('the', 667723), ('and', 324431), ('a', 322940), ('of', 289409)]\n",
      "Sample data [[279, 174, 436, 7043, 46, 3381, 43, 3, 15, 16, 223, 1129, 72, 1706, 1168, 37, 1, 1308, 1604, 2142, 836, 4, 61, 39, 25, 53, 172, 9, 41, 117, 23, 453, 44, 99, 4, 1, 101, 89, 4, 178, 27, 2988, 8, 1, 4177, 2, 7043, 12, 104, 25, 1654, 399, 22, 2, 92, 1499, 364, 73, 304, 33, 61, 57, 9, 117, 23, 453, 1, 104, 70, 142, 64, 453, 44, 6, 3, 53, 9632, 34861, 9793, 9265, 1, 444, 6, 24, 269, 122, 14, 516, 35, 1236, 24, 12, 127, 72, 244, 322, 184, 86, 2, 273, 56, 3788, 4, 3, 17393, 4177, 26, 62, 13243, 721, 5, 29, 1705, 121, 7043, 414, 51, 70, 25, 69, 501, 1, 302, 95, 218, 4, 10, 3876, 7043, 703, 178, 30, 44, 9265, 12, 2988, 70, 25, 556, 134, 9265, 6, 2100, 5, 29, 1, 118, 17, 56, 2517, 15439, 40, 11238, 56, 1314, 129, 7, 13, 30, 9, 96, 78, 5, 387, 37, 1592, 7, 122, 34, 519, 8], [10, 6, 34, 464, 4, 134, 1, 2219, 4, 205, 105, 25, 1, 168, 3860, 2, 347, 39, 12, 64, 163, 276, 149, 129, 3, 590, 447, 4, 1, 92, 1163, 14235, 1993, 4, 1774, 21, 2, 1774, 4941, 35, 137, 251, 13022, 107, 209, 121, 11, 32, 25, 2172, 4, 114, 2, 114, 69, 87, 21, 1246, 17, 10, 28, 141, 65, 169, 652, 518, 33194, 40, 103, 169, 835, 6081, 16, 1774, 21, 40, 18748, 1489, 1, 2806, 1980, 2827, 40, 2377, 16, 1774, 4941, 2, 65, 1, 145, 781, 1774, 21, 12, 2237, 7093, 408, 610, 164, 10, 19, 16791, 31, 1, 2647, 2, 9, 139, 131, 1528, 47, 1, 2130, 942, 5716, 13, 400, 8, 10, 19, 2, 134, 1, 2130, 125, 24, 212, 285, 1, 2525, 168, 104, 37, 2014, 18546, 171, 19, 9, 137, 108, 17, 942, 5716, 46, 86, 392, 1, 2525, 168, 2040, 104, 2, 31, 222, 8, 2014, 26, 104, 1061, 61, 90, 7, 661, 15987, 435, 10, 6, 334, 950, 205, 1140, 39, 25, 3542, 127, 105, 5, 65, 2, 45, 20, 64, 180, 5, 65, 10, 28, 103, 7820, 308, 61, 6, 2394, 3, 10703, 940, 18, 46, 127, 114, 2, 3, 127, 225, 1, 62, 148, 11, 90, 10, 31, 30, 276, 149, 13, 3, 531, 515, 22, 1, 349, 1, 619, 13, 216, 2509, 61, 269, 493, 5, 231, 55, 16, 1, 486, 19, 388, 18, 23, 181]]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(sentences):\n",
    "    words = ''.join(sentences).split()\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    \n",
    "    unk_count = 0\n",
    "    sent_data = []\n",
    "    for sentence in sentences:\n",
    "        data = []\n",
    "        for word in sentence.split():\n",
    "            if word in dictionary:\n",
    "                index = dictionary[word]\n",
    "            else:\n",
    "                index = 0  # dictionary['UNK']\n",
    "                unk_count = unk_count + 1\n",
    "            data.append(index)\n",
    "        sent_data.append(data)\n",
    "    \n",
    "    count[0][1] = unk_count\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return sent_data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(sentences)\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:2])\n",
    "# del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11508957\n"
     ]
    }
   ],
   "source": [
    "skip_window = 2\n",
    "instances = 0\n",
    "for sentence  in data:\n",
    "    instances += len(sentence)-2*skip_window\n",
    "print(instances)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skip_window = 2\n",
    "context = np.zeros((instances,skip_window*2),dtype=np.int32)\n",
    "labels = np.zeros((instances,1),dtype=np.int32)\n",
    "doc = np.zeros((instances,1),dtype=np.int32)\n",
    "\n",
    "k = 0\n",
    "for doc_id, sentence  in enumerate(data):\n",
    "    for i in range(skip_window, len(sentence)-skip_window):\n",
    "        buffer = sentence[i-skip_window:i+skip_window+1]\n",
    "        del buffer[skip_window]\n",
    "        labels[k] = sentence[i]\n",
    "        context[k] = buffer\n",
    "        doc[k] = doc_id\n",
    "        k += 1\n",
    "        \n",
    "shuffle_idx = np.random.permutation(k)\n",
    "labels = labels[shuffle_idx]\n",
    "doc = doc[shuffle_idx]\n",
    "context = context[shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "context_window = 2*skip_window\n",
    "embedding_size = 100 # Dimension of the embedding vector.\n",
    "softmax_width = embedding_size # +embedding_size2+embedding_size3\n",
    "num_sampled = 50 # Number of negative examples to sample.\n",
    "sum_ids = np.repeat(np.arange(batch_size),context_window)\n",
    "\n",
    "len_docs = len(data)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    # Input data.\n",
    "    train_word_dataset = tf.placeholder(tf.int32, shape=[batch_size*context_window])\n",
    "    train_doc_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "\n",
    "    segment_ids = tf.constant(sum_ids, dtype=tf.int32)\n",
    "\n",
    "    word_embeddings = tf.Variable(tf.random_uniform([vocabulary_size,embedding_size],-1.0,1.0))\n",
    "    doc_embeddings = tf.Variable(tf.random_uniform([len_docs,embedding_size],-1.0,1.0))\n",
    "\n",
    "    softmax_weights = tf.Variable(tf.truncated_normal([vocabulary_size, softmax_width],\n",
    "                             stddev=1.0 / np.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed_words = tf.segment_sum(tf.nn.embedding_lookup(word_embeddings, train_word_dataset),segment_ids)\n",
    "    embed_docs = tf.nn.embedding_lookup(doc_embeddings, train_doc_dataset)\n",
    "    embed = embed_words+embed_docs#+embed_hash+embed_users\n",
    "\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(tf.nn.nce_loss(softmax_weights, softmax_biases, embed,\n",
    "                                   train_labels, num_sampled, vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.AdagradOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embed_docs), 1, keep_dims=True))\n",
    "    normalized_doc_embeddings = embed_docs / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# Chunk the data to be passed into the tensorflow Model\n",
    "###########################\n",
    "data_idx = 0\n",
    "def generate_batch(batch_size):\n",
    "    global data_idx\n",
    "\n",
    "    if data_idx+batch_size<len_docs:\n",
    "        batch_labels = labels[data_idx:data_idx+batch_size]\n",
    "        batch_doc_data = doc[data_idx:data_idx+batch_size]\n",
    "        batch_word_data = context[data_idx:data_idx+batch_size]\n",
    "        data_idx += batch_size\n",
    "    else:\n",
    "        overlay = batch_size - (len_docs-data_idx)\n",
    "        batch_labels = np.vstack([labels[data_idx:len_docs],labels[:overlay]])\n",
    "        batch_doc_data = np.vstack([doc[data_idx:len_docs],doc[:overlay]])\n",
    "        batch_word_data = np.vstack([context[data_idx:len_docs],context[:overlay]])\n",
    "        data_idx = overlay\n",
    "    batch_word_data = np.reshape(batch_word_data,(-1,1))\n",
    "\n",
    "    return batch_labels, batch_word_data, batch_doc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 243.351120\n",
      "Average loss at step 10000: 38.311708\n",
      "Average loss at step 20000: 6.042729\n",
      "Average loss at step 30000: 1.952032\n",
      "Average loss at step 40000: 0.872490\n",
      "Average loss at step 50000: 0.570455\n",
      "Average loss at step 60000: 0.502472\n",
      "Average loss at step 70000: 0.480899\n",
      "Average loss at step 80000: 0.468121\n",
      "Average loss at step 90000: 0.462165\n",
      "Average loss at step 100000: 0.457372\n",
      "Average loss at step 110000: 0.456215\n",
      "Average loss at step 120000: 0.454574\n",
      "Average loss at step 130000: 0.452886\n",
      "Average loss at step 140000: 0.450593\n",
      "Average loss at step 150000: 0.449832\n",
      "Average loss at step 160000: 0.448379\n",
      "Average loss at step 170000: 0.447042\n",
      "Average loss at step 180000: 0.446207\n",
      "Average loss at step 190000: 0.446497\n",
      "Average loss at step 200000: 0.445484\n"
     ]
    }
   ],
   "source": [
    "num_steps = 200001\n",
    "step_delta = int(num_steps/20)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_labels, batch_word_data, batch_doc_data\\\n",
    "        = generate_batch(batch_size)\n",
    "        feed_dict = {train_word_dataset : np.squeeze(batch_word_data),\n",
    "                     train_doc_dataset : np.squeeze(batch_doc_data),\n",
    "                     train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % step_delta == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / step_delta\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            average_loss = 0\n",
    "\n",
    "    # Get the weights to save for later\n",
    "    final_doc_embeddings = normalized_doc_embeddings.eval()\n",
    "    final_word_embeddings = word_embeddings.eval()\n",
    "    final_word_embeddings_out = softmax_weights.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.414069\n"
     ]
    }
   ],
   "source": [
    "rand_doc = np.random.randint(len_docs)\n",
    "closest_doc = np.argmax(np.delete(final_doc_embeddings,rand_doc,0)\n",
    "                        .dot(final_doc_embeddings[rand_doc][:,None]))\n",
    "if closest_doc >= rand_doc:\n",
    "    closest_doc += 1\n",
    "    \n",
    "print(final_doc_embeddings[rand_doc].dot(final_doc_embeddings[closest_doc][:,None])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wow my first review of this movie was so negative that it was not excepted i will try to tone this one down lets be real no one wants to see a chuck norris movie where he is not the main character there was a good fight scene at the end but the rest of the movie stank i have to wonder if old chuck just can t hang with the best any more has he slowed down so much that he has to turn out junk like this and hope that his reputation will carry him through the entire movie chuck is an awesome martial artist and as we have seen from walker texas ranger a fairly good actor but the trick is to combine both of these qualities in his movies and this one does not very disappointing for us norris fans chuck stay as the main character in your movies because this does not work for you gary'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[rand_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is halfway to being a top movie the opening section which spoofs hollywood social message films is absolutely brilliant it is a riot from start to finish the second section which introduces us to the main characters of the story is really great too we get a lot of great comic setups top notch performances and the dialog is really dynamic spoiler warning the one think that really annoyed me about this film though is the ending which i think contradicts everything that went before my interpretation was that this film was taking the mickey out all the silly prejudices and innuendo of small town gossip and national tabloid sensationalism i loved that the film was championing the cause that a person s sexuality is not determined by their hobbies idiosyncrasies fashion sense or whatever and then the ending goes and re enforces all the gossip and stereotypes that the movie successfully lampooned in the first place it turns out everyone was right godamit this was very disappointing to what was actually a great story '"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[closest_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
